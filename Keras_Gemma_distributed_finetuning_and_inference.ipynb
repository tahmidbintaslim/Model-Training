{
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "Keras Gemma distributed finetuning and inference",
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "tpu1vmV38",
      "dataSources": [
        {
          "sourceId": 10262,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 5172
        }
      ],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 672.253242,
      "end_time": "2024-02-21T10:06:37.294427",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2024-02-21T09:55:25.041185",
      "version": "2.5.0"
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tahmidbintaslim/Model-Training/blob/main/Keras_Gemma_distributed_finetuning_and_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'gemma/keras/gemma_7b_en/1:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-models-data%2F5172%2F10262%2Fbundle%2Farchive.tar.gz%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240224%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240224T112919Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D7f398c9fadae8ee594cf90b4e1b54d97201a5139c3285181c1b97bd37c1cf3ee2c85aeb86b9a860984444779bae4cc018f51688c5fa71e2d7556158bede3319489cf7944a7df0d487295ce9bdb3bd7901c4ccb92eac314d31ee4a121fe5b24521a69c56314a8e67d52823d9b9f3732b7505ae8a0974f42bb17259cf336a9a60ee9ba71ff8578b2720c4d90a4a09898701ed97ef3bc606b8909e0ae1dfe9c270222383ec1926c9547335aecd029ab8dab31164d6c6700ff49e3e32e4e023df6a47f932c73b3bee5d977d293c42bf4e1da9723d16c56da3132e5762785db0757ab4bc81a867d62c226ce8db3862d8d1638fc9b4eec68556744b965c680b55159ce'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "brOS796q5F5O"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Copyright 2024 Google LLC."
      ],
      "metadata": {
        "id": "60KmTK7o6ppd",
        "papermill": {
          "duration": 0.007422,
          "end_time": "2024-02-21T09:55:26.786189",
          "exception": false,
          "start_time": "2024-02-21T09:55:26.778767",
          "status": "completed"
        },
        "tags": []
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "metadata": {
        "papermill": {
          "duration": 0.012321,
          "end_time": "2024-02-21T09:55:26.80406",
          "exception": false,
          "start_time": "2024-02-21T09:55:26.791739",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-21T10:57:58.683052Z",
          "iopub.execute_input": "2024-02-21T10:57:58.683407Z",
          "iopub.status.idle": "2024-02-21T10:57:58.688242Z",
          "shell.execute_reply.started": "2024-02-21T10:57:58.683376Z",
          "shell.execute_reply": "2024-02-21T10:57:58.687417Z"
        },
        "trusted": true,
        "id": "dDWPbOsG5F5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://ai.google.dev/gemma/docs/distributed_tuning\"><img src=\"https://ai.google.dev/static/site-assets/images/docs/notebook-site-button.png\" height=\"32\" width=\"32\" />View on ai.google.dev</a>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.kaggle.com/code/nilaychauhan/keras-gemma-distributed-finetuning-and-inference\"><img src=\"https://www.kaggle.com/static/images/logos/kaggle-logo-transparent-300.png\" height=\"32\" width=\"70\"/>Run in Kaggle</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/335\"><img src=\"https://ai.google.dev/images/cloud-icon.svg\" width=\"40\" />Open in Vertex AI</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/google/generative-ai-docs/blob/main/site/en/gemma/docs/distributed_tuning.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.005641,
          "end_time": "2024-02-21T09:55:26.815024",
          "exception": false,
          "start_time": "2024-02-21T09:55:26.809383",
          "status": "completed"
        },
        "tags": [],
        "id": "-oJ555nJ5F5V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Distributed tuning with Gemma using Keras"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.005634,
          "end_time": "2024-02-21T09:55:26.825916",
          "exception": false,
          "start_time": "2024-02-21T09:55:26.820282",
          "status": "completed"
        },
        "tags": [],
        "id": "_2KENSuF5F5V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "\n",
        "Gemma is a family of lightweight, state-of-the-art open models built from research and technology used to create Google Gemini models. Gemma can be further finetuned to suit specific needs. But Large Language Models, such as Gemma, can be very large in size and some of them may not fit on a sing accelerator for finetuning. In this case there are two general approaches for finetuning them:\n",
        "1. Parameter Efficient Fine-Tuning (PEFT), which seeks to shrink the effective model size by sacrificing some fidelity. LoRA falls in this category and the [Finetune Gemma models in Keras using LoRA](https://ai.google.dev/gemma/docs/lora_tuning) tutorial demonstrates how to finetune the Gemma 2B model `gemma_2b_en` with LoRA using KerasNLP on a single GPU.\n",
        "2. Full parameter finetuning with model parallelism. Model parallelism distributes a single model's weights across multiple devices and enables horizontal scaling. You can find out more about distributed training in this [Keras guide](https://keras.io/guides/distribution/).\n",
        "\n",
        "This tutorial walks you through using Keras with a JAX backend to finetune the Gemma 7B model with LoRA and model-parallism distributed training on Google's Tensor Processing Unit (TPU). Note that LoRA can be turned off in this tutorial for a slower but more accurate full-parameter tuning."
      ],
      "metadata": {
        "id": "Tdlq6K0znh3O",
        "papermill": {
          "duration": 0.005633,
          "end_time": "2024-02-21T09:55:26.83679",
          "exception": false,
          "start_time": "2024-02-21T09:55:26.831157",
          "status": "completed"
        },
        "tags": []
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using accelerators\n",
        "\n",
        "Technically you can use either TPU or GPU for this tutorial.\n",
        "\n",
        "### Notes on TPU environments\n",
        "\n",
        "Google has 3 products that provide TPUs:\n",
        "* [Colab](https://colab.sandbox.google.com/) provides TPU v2, which is not sufficient for this tutorial.\n",
        "* [Kaggle](https://www.kaggle.com/) offers TPU v3 for free and they work for this tutorial.\n",
        "* [Cloud TPU](https://cloud.google.com/tpu?hl=en) offers TPU v3 and newer generations. One way to set it up is:\n",
        "  1. Create a new [TPU VM](https://cloud.google.com/tpu/docs/managing-tpus-tpu-vm#tpu-vms)\n",
        "  2. Set up [SSH port forwarding](https://cloud.google.com/solutions/connecting-securely#port-forwarding-over-ssh) for your intended Jupyter server port\n",
        "  3. Install Jupyter and start it on the TPU VM, then connect to Colab through \"Connect to a local runtime\"\n",
        "\n",
        "### Notes on multi-GPU setup\n",
        "\n",
        "Although this tutorial focuses on the TPU use case, you can easily adapt it for your own needs if you have a multi-GPU machine.\n",
        "\n",
        "If you prefer to work through Colab, it's also possible to provision a multi-GPU VM for Colab directly through \"Connect to a custom GCE VM\" in the Colab Connect menu.\n",
        "\n",
        "\n",
        "We will focus on using the **free TPU from Kaggle** here."
      ],
      "metadata": {
        "id": "z-jBO5hmDwrc",
        "papermill": {
          "duration": 0.005425,
          "end_time": "2024-02-21T09:55:26.847611",
          "exception": false,
          "start_time": "2024-02-21T09:55:26.842186",
          "status": "completed"
        },
        "tags": []
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Before you begin"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.005257,
          "end_time": "2024-02-21T09:55:26.85841",
          "exception": false,
          "start_time": "2024-02-21T09:55:26.853153",
          "status": "completed"
        },
        "tags": [],
        "id": "QCXyHKIq5F5W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gemma setup\n",
        "\n",
        "To complete this tutorial, you will first need to complete the setup instructions at [Gemma setup](https://ai.google.dev/gemma/docs/setup). The Gemma setup instructions show you how to do the following:\n",
        "\n",
        "Gemma models are hosted by Kaggle. To use Gemma, request access on Kaggle:\n",
        "\n",
        "- Sign in or register at [kaggle.com](https://www.kaggle.com)\n",
        "- Open the [Gemma model card](https://www.kaggle.com/models/google/gemma) and select _\"Request Access\"_\n",
        "- Complete the consent form and accept the terms and conditions\n"
      ],
      "metadata": {
        "id": "aKvTsIkL98BG",
        "papermill": {
          "duration": 0.005259,
          "end_time": "2024-02-21T09:55:26.869282",
          "exception": false,
          "start_time": "2024-02-21T09:55:26.864023",
          "status": "completed"
        },
        "tags": []
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation\n",
        "\n",
        "Install Keras and KerasNLP with the Gemma model."
      ],
      "metadata": {
        "id": "AO7a1Q4Yyc9Z",
        "papermill": {
          "duration": 0.005175,
          "end_time": "2024-02-21T09:55:26.880069",
          "exception": false,
          "start_time": "2024-02-21T09:55:26.874894",
          "status": "completed"
        },
        "tags": []
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Keras 3 last. See https://keras.io/getting_started/ for more details.\n",
        "!pip install -q tensorflow-cpu\n",
        "!pip install -q -U keras-nlp tensorflow-hub\n",
        "!pip install -q -U keras>=3"
      ],
      "metadata": {
        "id": "WWEzVJR4Fx9g",
        "papermill": {
          "duration": 37.05282,
          "end_time": "2024-02-21T09:56:03.93859",
          "exception": false,
          "start_time": "2024-02-21T09:55:26.88577",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-21T10:57:58.689597Z",
          "iopub.execute_input": "2024-02-21T10:57:58.689869Z",
          "iopub.status.idle": "2024-02-21T10:59:01.818391Z",
          "shell.execute_reply.started": "2024-02-21T10:57:58.689842Z",
          "shell.execute_reply": "2024-02-21T10:59:01.817538Z"
        },
        "_kg_hide-output": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set up Keras JAX backend"
      ],
      "metadata": {
        "id": "fr9VnPm7FoMf",
        "papermill": {
          "duration": 0.006513,
          "end_time": "2024-02-21T09:56:03.951855",
          "exception": false,
          "start_time": "2024-02-21T09:56:03.945342",
          "status": "completed"
        },
        "tags": []
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import JAX and run a sanity check on TPU. Kaggle offers TPUv3-8 devices which have 8 TPU cores with 16GB of memory each."
      ],
      "metadata": {
        "id": "lbZsUvfhwL2D",
        "papermill": {
          "duration": 0.00599,
          "end_time": "2024-02-21T09:56:03.964473",
          "exception": false,
          "start_time": "2024-02-21T09:56:03.958483",
          "status": "completed"
        },
        "tags": []
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "\n",
        "jax.devices()"
      ],
      "metadata": {
        "id": "BK4MpHLKGujb",
        "papermill": {
          "duration": 8.126711,
          "end_time": "2024-02-21T09:56:12.097265",
          "exception": false,
          "start_time": "2024-02-21T09:56:03.970554",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-21T10:59:01.81969Z",
          "iopub.execute_input": "2024-02-21T10:59:01.820027Z",
          "iopub.status.idle": "2024-02-21T10:59:09.999308Z",
          "shell.execute_reply.started": "2024-02-21T10:59:01.819995Z",
          "shell.execute_reply": "2024-02-21T10:59:09.998494Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# The Keras 3 distribution API is only implemented for the JAX backend for now\n",
        "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
        "# Pre-allocate 90% of TPU memory to minimize memory fragmentation and allocation\n",
        "# overhead\n",
        "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"0.9\""
      ],
      "metadata": {
        "id": "WEgg_OVIL2HY",
        "papermill": {
          "duration": 0.01342,
          "end_time": "2024-02-21T09:56:12.117529",
          "exception": false,
          "start_time": "2024-02-21T09:56:12.104109",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-21T10:59:10.001136Z",
          "iopub.execute_input": "2024-02-21T10:59:10.001516Z",
          "iopub.status.idle": "2024-02-21T10:59:10.00542Z",
          "shell.execute_reply.started": "2024-02-21T10:59:10.001486Z",
          "shell.execute_reply": "2024-02-21T10:59:10.004726Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load model"
      ],
      "metadata": {
        "id": "wo1xkzr62hXN",
        "papermill": {
          "duration": 0.00603,
          "end_time": "2024-02-21T09:56:12.129531",
          "exception": false,
          "start_time": "2024-02-21T09:56:12.123501",
          "status": "completed"
        },
        "tags": []
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import keras_nlp"
      ],
      "metadata": {
        "id": "kFCmWEKdMA_Y",
        "papermill": {
          "duration": 8.153944,
          "end_time": "2024-02-21T09:56:20.315559",
          "exception": false,
          "start_time": "2024-02-21T09:56:12.161615",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-21T10:59:10.006337Z",
          "iopub.execute_input": "2024-02-21T10:59:10.006609Z",
          "iopub.status.idle": "2024-02-21T10:59:16.967281Z",
          "shell.execute_reply.started": "2024-02-21T10:59:10.006582Z",
          "shell.execute_reply": "2024-02-21T10:59:16.966424Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Notes on mixed precision training on NVIDIA GPUs\n",
        "\n",
        "When training on NVIDIA GPUs, mixed precision (`keras.mixed_precision.set_global_policy('mixed_bfloat16')`) can be used to speed up training with minimal effect on training quality. In most case, it is recommended to turn on mixed precision as it saves both memory and time. However, be aware that at small batch sizes, it can inflate memory usage by 1.5x (weights will be loaded twice, at half precision and full precision).\n",
        "\n",
        "For inference, half-precision (`keras.config.set_floatx(\"bfloat16\")`) will work and save memory while mixed-precision is not applicable."
      ],
      "metadata": {
        "id": "bx3m8f1dB7nk",
        "papermill": {
          "duration": 0.005945,
          "end_time": "2024-02-21T09:56:20.327808",
          "exception": false,
          "start_time": "2024-02-21T09:56:20.321863",
          "status": "completed"
        },
        "tags": []
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment the line below if you want to enable mixed precision training on GPUs\n",
        "# keras.mixed_precision.set_global_policy('mixed_bfloat16')"
      ],
      "metadata": {
        "id": "T0lHxEDX03gp",
        "papermill": {
          "duration": 0.0122,
          "end_time": "2024-02-21T09:56:20.345701",
          "exception": false,
          "start_time": "2024-02-21T09:56:20.333501",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-21T10:59:16.968232Z",
          "iopub.execute_input": "2024-02-21T10:59:16.968668Z",
          "iopub.status.idle": "2024-02-21T10:59:16.971979Z",
          "shell.execute_reply.started": "2024-02-21T10:59:16.968639Z",
          "shell.execute_reply": "2024-02-21T10:59:16.971166Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To load the model with the weights and tensors distributed across TPUs, first create a new `DeviceMesh`. `DeviceMesh` represents a collection of hardware devices configured for distributed computation and was introduced in Keras 3 as part of the unified distribution API.\n",
        "\n",
        "The distribution API enables data and model parallelism, allowing for efficient scaling of deep learning models on multiple accelerators and hosts. It leverages the underlying framework (e.g. JAX) to distribute the program and tensors according to the sharding directives through a procedure called single program, multiple data (SPMD) expansion. Check out more details in the new [Keras 3 distribution API guide](https://keras.io/guides/distribution/)."
      ],
      "metadata": {
        "id": "xrR8TpVS6uPs",
        "papermill": {
          "duration": 0.005852,
          "end_time": "2024-02-21T09:56:20.357594",
          "exception": false,
          "start_time": "2024-02-21T09:56:20.351742",
          "status": "completed"
        },
        "tags": []
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a device mesh with (1, 8) shape so that the weights are sharded across\n",
        "# all 8 TPUs.\n",
        "device_mesh = keras.distribution.DeviceMesh(\n",
        "    (1, 8),\n",
        "    [\"batch\", \"model\"],\n",
        "    devices=keras.distribution.list_devices())"
      ],
      "metadata": {
        "id": "7gxEkpUiP1Qf",
        "papermill": {
          "duration": 0.012454,
          "end_time": "2024-02-21T09:56:20.375812",
          "exception": false,
          "start_time": "2024-02-21T09:56:20.363358",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-21T10:59:16.973164Z",
          "iopub.execute_input": "2024-02-21T10:59:16.973387Z",
          "iopub.status.idle": "2024-02-21T10:59:16.99772Z",
          "shell.execute_reply.started": "2024-02-21T10:59:16.973356Z",
          "shell.execute_reply": "2024-02-21T10:59:16.996977Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`LayoutMap` from the distribution API specifies how the weights and tensors should be sharded or replicated, using the string keys, for example, `token_embedding/embeddings` below, which are treated like regex to match tensor paths. Matched tensors are sharded with model dimensions (8 TPUs); others will be fully replicated."
      ],
      "metadata": {
        "id": "gTSJUwkC-7c6",
        "papermill": {
          "duration": 0.00582,
          "end_time": "2024-02-21T09:56:20.387661",
          "exception": false,
          "start_time": "2024-02-21T09:56:20.381841",
          "status": "completed"
        },
        "tags": []
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_dim = \"model\"\n",
        "\n",
        "layout_map = keras.distribution.LayoutMap(device_mesh)\n",
        "\n",
        "# Weights that match 'token_embedding/embeddings' will be sharded on 8 TPUs\n",
        "layout_map[\"token_embedding/embeddings\"] = (None, model_dim)\n",
        "# Regex to match against the query, key and value matrices in the decoder\n",
        "# attention layers\n",
        "layout_map[\"decoder_block.*attention.*(query|key|value).*kernel\"] = (\n",
        "    None, model_dim, None)\n",
        "\n",
        "layout_map[\"decoder_block.*attention_output.*kernel\"] = (\n",
        "    None, None, model_dim)\n",
        "layout_map[\"decoder_block.*ffw_gating.*kernel\"] = (model_dim, None)\n",
        "layout_map[\"decoder_block.*ffw_linear.*kernel\"] = (None, model_dim)"
      ],
      "metadata": {
        "id": "8Wgh8h0qQCcu",
        "papermill": {
          "duration": 0.013198,
          "end_time": "2024-02-21T09:56:20.406598",
          "exception": false,
          "start_time": "2024-02-21T09:56:20.3934",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-21T10:59:16.998667Z",
          "iopub.execute_input": "2024-02-21T10:59:16.998919Z",
          "iopub.status.idle": "2024-02-21T10:59:17.00903Z",
          "shell.execute_reply.started": "2024-02-21T10:59:16.998894Z",
          "shell.execute_reply": "2024-02-21T10:59:17.008286Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`ModelParallel` allows you to shard model weights or activation tensors across all devcies on the `DeviceMesh`. In this case, some of the Gemma 7B model weights are sharded across 8 TPU chips according the `layout_map` defined above. Now load the model in the distributed way."
      ],
      "metadata": {
        "id": "6n4Zlvk9ALhZ",
        "papermill": {
          "duration": 0.005938,
          "end_time": "2024-02-21T09:56:20.418485",
          "exception": false,
          "start_time": "2024-02-21T09:56:20.412547",
          "status": "completed"
        },
        "tags": []
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_parallel = keras.distribution.ModelParallel(\n",
        "    device_mesh, layout_map, batch_dim_name=\"batch\")\n",
        "\n",
        "keras.distribution.set_distribution(model_parallel)\n",
        "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_7b_en\")"
      ],
      "metadata": {
        "id": "bu48vUnbQj0p",
        "papermill": {
          "duration": 145.668669,
          "end_time": "2024-02-21T09:58:46.092826",
          "exception": false,
          "start_time": "2024-02-21T09:56:20.424157",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-21T10:59:17.009943Z",
          "iopub.execute_input": "2024-02-21T10:59:17.010181Z",
          "iopub.status.idle": "2024-02-21T11:01:46.896327Z",
          "shell.execute_reply.started": "2024-02-21T10:59:17.010159Z",
          "shell.execute_reply": "2024-02-21T11:01:46.895225Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now verify that the model has been partitioned correctly. Let's take `decoder_block_1` as an example."
      ],
      "metadata": {
        "id": "ORCOIawAvpZ1",
        "papermill": {
          "duration": 0.006357,
          "end_time": "2024-02-21T09:58:46.105898",
          "exception": false,
          "start_time": "2024-02-21T09:58:46.099541",
          "status": "completed"
        },
        "tags": []
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_block_1 = gemma_lm.backbone.get_layer('decoder_block_1')\n",
        "print(type(decoder_block_1))\n",
        "for variable in decoder_block_1.weights:\n",
        "  print(f'{variable.path:<58}  {str(variable.shape):<16}  {str(variable.value.sharding.spec)}')"
      ],
      "metadata": {
        "id": "DqT7TRHKvoMK",
        "papermill": {
          "duration": 0.014281,
          "end_time": "2024-02-21T09:58:46.126669",
          "exception": false,
          "start_time": "2024-02-21T09:58:46.112388",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-21T11:01:46.898853Z",
          "iopub.execute_input": "2024-02-21T11:01:46.899153Z",
          "iopub.status.idle": "2024-02-21T11:01:46.904593Z",
          "shell.execute_reply.started": "2024-02-21T11:01:46.899125Z",
          "shell.execute_reply": "2024-02-21T11:01:46.903716Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference before finetuning"
      ],
      "metadata": {
        "id": "jc0ZzYIW0TSN",
        "papermill": {
          "duration": 0.005944,
          "end_time": "2024-02-21T09:58:46.139044",
          "exception": false,
          "start_time": "2024-02-21T09:58:46.1331",
          "status": "completed"
        },
        "tags": []
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gemma_lm.generate(\"Best comedy movies in the 90s \", max_length=64)"
      ],
      "metadata": {
        "id": "ClaTyBp3Tgr4",
        "papermill": {
          "duration": 26.22201,
          "end_time": "2024-02-21T09:59:12.367352",
          "exception": false,
          "start_time": "2024-02-21T09:58:46.145342",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-21T11:01:46.905574Z",
          "iopub.execute_input": "2024-02-21T11:01:46.905831Z",
          "iopub.status.idle": "2024-02-21T11:02:12.509872Z",
          "shell.execute_reply.started": "2024-02-21T11:01:46.905807Z",
          "shell.execute_reply": "2024-02-21T11:02:12.508708Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The model generates a list of great comedy movies from the 90s to watch. Now we finetune the Gemma model to change the output style."
      ],
      "metadata": {
        "id": "tKUXYLW_0lAx",
        "papermill": {
          "duration": 0.006264,
          "end_time": "2024-02-21T09:59:12.38068",
          "exception": false,
          "start_time": "2024-02-21T09:59:12.374416",
          "status": "completed"
        },
        "tags": []
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finetune with IMDB"
      ],
      "metadata": {
        "id": "IcPCXCwvXC7t",
        "papermill": {
          "duration": 0.006625,
          "end_time": "2024-02-21T09:59:12.393943",
          "exception": false,
          "start_time": "2024-02-21T09:59:12.387318",
          "status": "completed"
        },
        "tags": []
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "imdb_train = tfds.load(\n",
        "    \"imdb_reviews\",\n",
        "    split=\"train\",\n",
        "    as_supervised=True,\n",
        "    batch_size=2,\n",
        ")\n",
        "# Drop labels.\n",
        "imdb_train = imdb_train.map(lambda x, y: x)\n",
        "\n",
        "imdb_train.unbatch().take(1).get_single_element().numpy()"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "id": "6MVJlsuSXCcf",
        "papermill": {
          "duration": 43.816972,
          "end_time": "2024-02-21T09:59:56.217135",
          "exception": false,
          "start_time": "2024-02-21T09:59:12.400163",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-21T11:02:12.511083Z",
          "iopub.execute_input": "2024-02-21T11:02:12.511452Z",
          "iopub.status.idle": "2024-02-21T11:02:52.780431Z",
          "shell.execute_reply.started": "2024-02-21T11:02:12.511418Z",
          "shell.execute_reply": "2024-02-21T11:02:52.779492Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a subset of the dataset for faster training.\n",
        "imdb_train = imdb_train.take(2000)"
      ],
      "metadata": {
        "id": "N-04czr3XTTl",
        "papermill": {
          "duration": 0.035807,
          "end_time": "2024-02-21T09:59:56.283455",
          "exception": false,
          "start_time": "2024-02-21T09:59:56.247648",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-21T11:02:52.78163Z",
          "iopub.execute_input": "2024-02-21T11:02:52.781951Z",
          "iopub.status.idle": "2024-02-21T11:02:52.786952Z",
          "shell.execute_reply.started": "2024-02-21T11:02:52.78192Z",
          "shell.execute_reply": "2024-02-21T11:02:52.785735Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform finetuning using [Low Rank Adaptation](https://arxiv.org/abs/2106.09685) (LoRA). LoRA is a fine-tuning technique which greatly reduces the number of trainable parameters for downstream tasks by freezing the full weights of the model and inserting a smaller number of new trainable weights into the model. Basically LoRA reparameterizes the larger full weight matrices by 2 smaller low-rank matrices AxB to train and this technique makes training much faster and more memory-efficient."
      ],
      "metadata": {
        "id": "xiLW0SpI1PfC",
        "papermill": {
          "duration": 0.032799,
          "end_time": "2024-02-21T09:59:56.345251",
          "exception": false,
          "start_time": "2024-02-21T09:59:56.312452",
          "status": "completed"
        },
        "tags": []
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable LoRA for the model and set the LoRA rank to 4.\n",
        "gemma_lm.backbone.enable_lora(rank=4)"
      ],
      "metadata": {
        "id": "3o_Gi3v_jp7s",
        "papermill": {
          "duration": 0.527031,
          "end_time": "2024-02-21T09:59:56.902475",
          "exception": false,
          "start_time": "2024-02-21T09:59:56.375444",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-21T11:02:52.787968Z",
          "iopub.execute_input": "2024-02-21T11:02:52.788243Z",
          "iopub.status.idle": "2024-02-21T11:02:53.28383Z",
          "shell.execute_reply.started": "2024-02-21T11:02:52.788216Z",
          "shell.execute_reply": "2024-02-21T11:02:53.282871Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tune on the IMDb movie reviews dataset.\n",
        "\n",
        "# Limit the input sequence length to 128 to control memory usage.\n",
        "gemma_lm.preprocessor.sequence_length = 128\n",
        "# Use AdamW (a common optimizer for transformer models).\n",
        "optimizer = keras.optimizers.AdamW(\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "# Exclude layernorm and bias terms from decay.\n",
        "optimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n",
        "\n",
        "gemma_lm.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=optimizer,\n",
        "    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "gemma_lm.summary()\n",
        "gemma_lm.fit(imdb_train, epochs=1)"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "id": "1-hQFy7hXWRl",
        "papermill": {
          "duration": 362.718929,
          "end_time": "2024-02-21T10:05:59.651432",
          "exception": false,
          "start_time": "2024-02-21T09:59:56.932503",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-21T11:02:53.284907Z",
          "iopub.execute_input": "2024-02-21T11:02:53.285195Z",
          "iopub.status.idle": "2024-02-21T11:08:54.048232Z",
          "shell.execute_reply.started": "2024-02-21T11:02:53.285167Z",
          "shell.execute_reply": "2024-02-21T11:08:54.047287Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that enabling LoRA reduces the number of trainable parameters significantly, from 7 billion to only 11 million."
      ],
      "metadata": {
        "id": "CnpeavB4fZ7Y",
        "papermill": {
          "duration": 0.137841,
          "end_time": "2024-02-21T10:05:59.925998",
          "exception": false,
          "start_time": "2024-02-21T10:05:59.788157",
          "status": "completed"
        },
        "tags": []
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference after finetuning"
      ],
      "metadata": {
        "id": "lBiOKlAy2MAe",
        "papermill": {
          "duration": 0.137064,
          "end_time": "2024-02-21T10:06:00.201917",
          "exception": false,
          "start_time": "2024-02-21T10:06:00.064853",
          "status": "completed"
        },
        "tags": []
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gemma_lm.generate(\"Best comedy movies in the 90s \", max_length=64)"
      ],
      "metadata": {
        "id": "9yNyJ8CLXfw0",
        "papermill": {
          "duration": 31.150786,
          "end_time": "2024-02-21T10:06:31.488004",
          "exception": false,
          "start_time": "2024-02-21T10:06:00.337218",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-02-21T11:08:54.049599Z",
          "iopub.execute_input": "2024-02-21T11:08:54.050247Z",
          "iopub.status.idle": "2024-02-21T11:09:24.877718Z",
          "shell.execute_reply.started": "2024-02-21T11:08:54.050213Z",
          "shell.execute_reply": "2024-02-21T11:09:24.876849Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After finetuning, the model has learned the style of movie reviews and is now generating output in that style in the context of 90s comedy movies."
      ],
      "metadata": {
        "id": "inqB1e_v0xP5",
        "papermill": {
          "duration": 0.136614,
          "end_time": "2024-02-21T10:06:31.761087",
          "exception": false,
          "start_time": "2024-02-21T10:06:31.624473",
          "status": "completed"
        },
        "tags": []
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What's next\n",
        "\n",
        "In this tutorial, you learned how to using KerasNLP JAX backend to finetune a Gemma model on the IMDb dataset in a distributed manner on the powerful TPUs. Here are a few suggestions for what else to learn:\n",
        "\n",
        "* Learn how to [get started with Keras Gemma](https://ai.google.dev/gemma/docs/get_started).\n",
        "* Learn how to [finetune the Gemma model on GPU](https://ai.google.dev/gemma/docs/lora_tuning)."
      ],
      "metadata": {
        "id": "bzKsCGIN0yX5",
        "papermill": {
          "duration": 0.148912,
          "end_time": "2024-02-21T10:06:32.078788",
          "exception": false,
          "start_time": "2024-02-21T10:06:31.929876",
          "status": "completed"
        },
        "tags": []
      }
    }
  ]
}